# 2.6 网络拥塞控制原理与实践

本节，我们来了解拥塞控制的原理，分析新一代的拥塞控制算法 BBR（Bottleneck Bandwidth and Round-trip propagation time，瓶颈带宽和往返传播时间算法）设计，并对比 BBR 与传统拥塞算法的效率。
## 2.6.1 网络拥塞控制原理

当网络中的数据传输量超过了网络的带宽容量时，会出现数据传输缓慢、数据包丢失，这种情况叫做拥塞（congestion）。若不对拥塞进行控制，网络会因为过载变得完全不可用。

图 2-22 展示了拥塞的产生和控制逻辑，其本质是控制图中横轴 inflight 数据量。首先，解释图中的一些术语：

- RTprop (Round-Trip propagation time，两个节点之间最小时延)：两个节点之间的最小时延，取决于物理距离，距离越长，时延越大。
- BtlBw（Bottleneck Bandwidth，瓶颈带宽）：如果把网络链路想象成水管，RTprop 就是水管的长度，BtlBw 则是水管最窄处的直径。
- BDP（Bandwidth-Delay Product，带宽和延迟的乘积）：它代表了网络上能够同时容纳的数据量（水管中有多少流动的水）。 BDP 的计算公式是：BDP = 带宽 × 延迟。其中，带宽以比特每秒（bps）为单位，延迟以秒为单位。
- inflight 数据：指已经发送出去但尚未收到确认的数据包。这些数据包仍在网络中传输，等待接收方的处理或确认。。

:::center
  ![](../assets/bbr-cc.png)<br/>
 图 2-22 网络拥塞的产生和控制逻辑
:::

inflight 数据受 RTT 和传输速率的影响，分为三个区间：

1. (0，BDP)：称为应用受限区（app limited）。在这个区间内，inflight 数据量未占满瓶颈带宽。
2. (BDP，BtlBwBuffSize)：称为带宽受限区（bandwidth limited）。在这个区间内，inflight 数据量已达到链路瓶颈容量，但尚未超过瓶颈容量加缓冲区容量。此时，应用能发送的数据量主要受带宽限制。
3. (BDP + BtlBwBuffSize，infinity)：称为缓冲区受限区（buffer limited）。在这个区间内，实际发送速率已超过瓶颈容量加缓冲区容量，超出部分的数据会被丢弃，从而产生丢包。

根据图 2-22，可以看出，**拥塞本质是 inflight 数据量持续向右侧偏离 BDP 线的行为，而拥塞控制就是控制 inflight 数据量偏离 BDP 线程度的方案或算法**。

## 2.6.2 早期拥塞控制旨在收敛

早期互联网的拥塞控制以丢包为控制条件，控制逻辑如图 2-21 所示。首先是慢启动（Slow Start），当出现丢包时，进行拥塞避免（Congestion Avoiance）阶段；当丢包不再出现时，再次进入慢启动阶段，如此一直反复。

:::center
  ![](../assets/cc.png)<br/>
 图 2-21 早期以丢包为条件的拥塞控制
:::

以丢包为控制条件的机制适应了早期互联网的特征：低带宽和浅缓存队列。但随着移动互联网的快速发展，尤其是图片和音视频应用的普及，使网络负载大幅增加。同时摩尔定律也推动设备的廉价化和性能提升，当路由器和网关的缓存队列增加，链路变得更长更宽时，传统的以丢包为控制条件的拥塞控制机制就不再适用。


## 2.6.3 现代拥塞控制旨在效能最大化

如果说初代的拥塞控制目标旨在收敛，防止互联网服务产生“拥塞崩溃”，那么这一次 BBR 的目标则旨在**充分利用链路带宽、路由/网关设备的缓存队列，使网络效能最大化**。


网络效能最大化的前提是找到网络传输中的最优点。图 2-22 中的两个黑色圆圈即代表网络传输的最优点：
- 上面的圆圈为 min RTT（延迟极小值），网络中路由/网关设备的 Buffer 未占满，没有任何丢包情况;
- 下面圆圈的为 max BW（带宽极大值），网络中路由/网关设备的 Buffer 被充分利用。

当网络传输处于最优点时：

- 数据包投递率 = BtlBW（瓶颈带宽），保证了瓶颈链路被 100% 利用；
- 在途数据包总数 = BDP（时延带宽积），保证未占用 Buffer。

然而，延迟极小值和带宽极大值互相悖论，无法同时测得。如图 2-24 所示，要测量最大带宽，就要把瓶颈链路填满，此时 Buffer 中有一定量的数据包，影响延迟指标。要测量最低延迟，就要保证 Buffer 为空，此时就无法测量最大带宽值。

:::center
  ![](../assets/bbr-2.png)<br/>
 图 2-24 无法同时得到 max BW 和 min RTT
:::

## 2.6.4 BBR 的设计原理

BBR 的解题思路是不再考虑丢包作为拥塞的判断条件，而是交替测量带宽和延迟。它使用一段时间内的带宽最大值和延迟最小值来估计发包速率，控制稳定发包速度，尽量榨干带宽，却又不让数据在中间设备的缓存队列上累积：

- 为了榨干带宽，BBR 周期性地探测链路条件是否变好了，如果是，则加大发送速率；
- 为了不让数据在中间设备的缓存队列上累积，BBR 会周期性地探测链路的最小 RTT，并使用最小 RTT 计算发包速率。

BBR 的拥塞控制状态机是实现上述思路的基础。BBR 的状态机在任何时刻都处于以下四个状态之一：启动（STARTUP）、排空（DRAIN）、带宽探测（PROBE_BW）和时延探测（PROBE_RTT）。它们之间的关系如图 2-23 所示。

- 启动阶段（STARTUP）：当连接建立时，BBR 采用传统拥塞控制算法慢启动方式，指数级增加发送速率，目的是尽可能快的探测到带宽极大值。当判断连续时间内发送速率不再增长时，说明已到达瓶颈带宽，此时状态切换至排空阶段。
- 排空阶段（DRAIN）：该阶段指数级降低发送速率，相当于启动阶段的逆过程，目的是将多占的 Buffer 慢慢排空。
- 完成以上两个阶段后，BBR 进入带宽探测阶段（PROBE_ BW），BBR 大部分时间都在该状态运行。当 BBR 测量到带宽极大值和延迟极小值，并且网络中在途数据包 inflight[^3] 等于 BDP 时，便开始以一个稳定的匀速维护着网络状态，偶尔小幅提速探测是否有更大带宽，偶尔小幅降速公平的让出部分带宽。
- 时延探测阶段（Probe RTT）：如果估计延迟不变(未测量到比上周期最小 RTT 更小值)，就进入延迟探测阶段。该状态下，拥塞窗口 Cwnd 被设置为 4 个 MSS（Maximum Segment Size，最大报文长度），并对 RTT 重新测量，持续 200ms，超时后，根据网络带宽是否满载决定状态切换为启动阶段或带宽探测阶段。


:::center
  ![](../assets/bbr-status.png)<br/>
 图 2-23 BBR 状态转移关系
:::

## 2.6.5 BBR 的应用

Linux 内核从 4.9 开始集成了 BBR 拥塞控制算法，此后，只要几个命令就能使用 BBR 提升网络吞吐。

1. 首先，查询系统所支持的拥塞控制算法。
```bash
$ sysctl net.ipv4.tcp_available_congestion_control
net.ipv4.tcp_congestion_control = bbr cubic reno
```
上面返回的结果中，显示当前系统支持 bbr cubic reno 三种拥塞控制算法。

2. 查询当前使用的拥塞控制算法。

```bash
$ sysctl net.ipv4.tcp_congestion_control
net.ipv4.tcp_congestion_control = cubic
```
绝大部分 Linux 系统默认的拥塞控制算法为 Cubic 算法。

3. 指定拥塞控制算法为 bbr。
```bash
$ echo net.ipv4.tcp_congestion_control=bbr >> /etc/sysctl.conf && sysctl -p
```
网络拥塞控制是单向生效，也就是说作为下行方的服务端调整了，客户端与服务端之间的网络吞吐即可提升。

拥塞控制算法设置为 BBR 之后，我们可以使用 tc 工具模拟真实的网络环境，测试 BBR 的效果。

下面，使用 tc 工具设置两台服务器的收/发增加 25ms 的延迟以及 1% 的丢包率。

```bash
$ tc qdisc add dev eth0 root netem loss 1% latency 25ms
```



## 2.6.6 BBR 性能表现

BBR 是迄今为止跨越不同路由发送数据的最快的算法，它尤其适合在存在一定丢包率的长链路环境下使用。

接下来，我们通过 tc 工具模拟弱网环境，对不同的拥塞控制算法进行网络吞吐量测试。如下所示，设置 eth0 网络接口上的流量的丢包率设置为 1% 和延迟设置为 25 毫秒。

```bash
$ tc qdisc add dev eth0 root netem loss 1% latency 25ms
```

在客户端节点使用 iperf3 工具测试两个主机之间的网络传输性能。下面 iperf3 参数 -c 的意思是，以客户端模式运行，请求 10.0.1.188 服务端的 8080 端口。

```bash
$ iperf3 -c 10.0.1.188 -p 8080
```

测试结果如表 2-3 所示，可以看出，BBR 在轻微丢包的网络环境下表现尤为出色。

:::center
表 2-3 各拥塞控制算法在不同丢包率环境下的性能测试 [表数据来源](https://toonk.io/tcp-bbr-exploring-tcp-congestion-control/index.html)
:::

|网络吞吐|拥塞控制算法（服务端）|延迟|丢包率|
|:--|:--|:--|:--|
|2.35Gb/s| Cubic| <1ms| 0% |
|195 Mb/s| Reno| <140ms| 0% |
|147 Mb/s| Cubic| <140ms| 0% |
|344 Mb/s| Westwood| <140ms| 0% |
|340 Mb/s| BBR| <140ms| 0% |
|1.13 Mb/s| Reno| <140ms| 1.5% |
|1.23 Mb/s| Cubic| <140ms| 1.5% |
|2.46 Mb/s| Westwood| <140ms| 1.5% |
|**160 Mb/s**| BBR| <140ms| 1.5% |
|0.65 Mb/s| Reno| <140ms| 3% |
|0.78 Mb/s| Cubic| <140ms| 3% |
|0.97 Mb/s| Westwood| <140ms| 3% |
|**132 Mb/s**| BBR| <140ms| 3% |


