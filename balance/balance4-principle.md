# 4.3.1 四层负载均衡工作模式

LVS（Linux Virtual Server，Linux 虚拟服务器）是四层负载的典型实现。LVS 通过虚拟化的方式将多台物理服务器（也称为 RealServer 真实服务器）组合成一个对外表现为单一虚拟服务器。现在 LVS 已经是Linux 内核的一部分，它完全在内核空间处理负载均衡和转发逻辑，一台普通配置的服务器可支持上百万个并发连接请求，具有很高的处理性能。

LVS 通过修改报文中的 IP 地址和端口来实现网络流量的分发。区别报文修改的方式，LVS 将工作模式分为 NAT（网络地址转换）模式、DR（Direct Routing，直接路由）模式 和 TUN（IP 隧道）模式。

## 1. 链路层负载均衡

LVS 的 DR（Direct Routing，直接路由）模式是一种链路层（L2）负载均衡技术。

链路层负载均衡的原理是，**修改请求数据帧中的目标 MAC 地址，让原本发送给负载均衡器请求的数据帧，再被二层交换机转发至服务器集群中对应的真实服务器（Real Server）**，这样真实服务器就获得了一个原本目标并不是发送给它的数据帧。

- 由于只修改了链路层数据帧的 MAC 地址，IP 层的数据没有任何变化。所以，被真实服务器正常接收。
- 由于 IP 数据包中包含了源（客户端）和目的地（负载均衡器）IP 地址。所以，真实服务器需保证自己的 IP 与 数据包中的目的地 IP 地址一致，数据包才能继续被处理。

综上所述，使用 DR 负载均衡模式时，需要把真实服务器的 VIP（Vritual IP，虚拟 IP）配置成和负载均衡器的 VIP 一样，LVS 配置真实服务器的原因也是如此，如下代码示例。

```bash
// 真实服务器配置
$ ifconfig lo:0 1.1.1.1 netmask 255.255.255.255 up // 绑定 VIP 地址在 lo 接口上才能收到目标地址为 VIP 的包。
$ route add -host 1.1.1.1 dev lo  // 目标地址是 VIP 的数据包从本机 lo 接口发送出去
```

DR 模式只有请求经过负载均衡器，而真实服务器的响应无需从负载均衡器原路返回。整个工作模式中，请求、转发、响应的链路形成一个“三角关系”。因此，DR 模式也被形象地称为“三角传输模式”，如图 4-7 所示。

:::center
  ![](../assets/balancer4-dsr.svg)<br/>
 图 4-7 链路层 DSR 模式负载均衡
:::

DR 模式的主要优势是，一些场景中响应的流量要远远大于请求的流量。如典型的 HTTP request/response 模式。假设请求占 10% 的流量，响应占 90%，使用三角传输模式，只需 1/10 的带宽就可以满足系统需求，这种模式极大节省了带宽成本，还提高了负载均衡器的可靠性（流量越低肯定越好）。

DR 模式效率很高，但它的缺陷也很明显：
- **因为响应不再经过负载均衡器，负载均衡器就无法知道 TCP 连接的完整状态，防火墙策略会受到影响**（TCP 连接只有 SYN，没有 ACK）。
- 其次，由于是数据链路层通信，负载均衡器和真实服务器必须在同一个子网内，受到网络侧的约束很大。

总结 DR 模式的优势（效率高）和劣势（不能跨子网）共同决定了该模式最适合作为数据中心的第一层均衡设备，用来连接其他的下级负载均衡器。

## 2. 网络层负载均衡

网络层负载均衡操作的是 IP 数据包，我们先对 IP 数据包有个简单的了解。

以 IP 协议为例，一个 IP 数据包由头部（Header）和载荷（Payload）组成，Header 内部包含版本、源地址、目的地地址等信息，结构如图 4-8 所示。

本文，我们无需太关注 Header 头部信息，只要知道 Header 内有源地址（Source address）和目的地地址（Destination address）即可。

:::center
  ![](../assets/ip.svg)<br/>
 图 4-8 ip 数据包结构
:::

**既然数据链层负载均衡可以通过改写 MAC 地址来实现数据包转发，到了网络层，我们也可以继续沿用改写 MAC 相似的思路修改网络层的 IP 数据包地址信息来实现数据转发**。

LVS 的 Tunnel（隧道）、NAT 模式都属于网络层负载均衡，只不过因为对 IP 数据包的不同形式修改而分成两种，我们先来看第一种修改方式。

### 2.1 隧道模式

第一种保持源数据包不变，新建一个 IP 数据包，将原来的 IP 数据包整体放进新 IP 数据包的 Payload 内，再通过三层交换机发送出去，真实服务器收到包之后，有一个对应拆包的机制，把负载均衡器自动添加的那层 Header 删掉，解析出 Payload 内部的 IP 数据包再进行正常处理。

**把一个数据包封装在另一个数据包内，其实就是一种隧道技术，譬如 Linux 中的隧道技术 ipip 就是字面 IP in IP 的意思。由于 IP 隧道工作在网络层，因此摆脱了直接路由模式的网络约束，所以 LVS Tunnel（隧道）模式可以跨越 VLAN**。

由于没有修改源数据包的任何信息，所以 IP 隧道模式仍具有三角传输模式的特点，即负载均衡转发器转发进来的请求，真实服务器去响应请求，IP 隧道模式从请求到响应的过程如图 4-9 所示。

:::center
  ![](../assets/balancer4-tunnel.svg)<br/>
图 4-9 Tunnel 模式
:::

隧道模式相当于 DR 模式的升级（支持了跨网），不过由于使用隧道模式：
- 所以要求真实服务器支持隧道协议，真实服务器只局限在部分 Linux 系统上；
- 其次，只要是三角模式（LVS 的 DR 模式或者 Tunnel 模式）必须要保证真实服务器与负载均衡服务器有相同的虚拟 IP 地址，因为回复客户端时，必须使用这个虚拟的 IP 作为数据包的源地址，这样客户端收到数据包之后才能正常解析；
- 最后，因为真实服务器和客户端对接，所以真实服务器得能访问外网。

### 2.2 NAT 模式

另一种对 IP 数据包的改写方式是，修改其头部中的目标地址（Destination address），将其更改为真实服务器的地址。修改后，用户原本发送给负载均衡器的数据包会被三层交换机转发到真实服务器的网卡上。

这段解释可能不够直观，但相信大多数读者都曾操作过类似的配置。例如，在家中设置路由器时，你希望外部设备可以访问家中某台运行服务器的电脑。假设家中的电脑 IP 是 192.168.1.100，并在端口 8080 上运行一个 Web 服务。你可以在路由器中设置端口转发（NAT），将外部访问路由器的 80 端口的请求转发到该电脑的 192.168.1.100:8080。这样，当外部设备通过路由器的公共 IP 访问 80 端口时，实际上就会连接到局域网内的服务器。

四层负载均衡器对 IP 数据包的改写与路由器中的“端口转发”原理相同。因此，这种负载均衡方式被称为 NAT 模式，其请求和响应的流程如图 4-10 所示。

:::center
  ![](../assets/balancer4-NAT.svg)<br/>
图 4-10 NAT 模式负载均衡
:::

举例一个具体的例子，假设客户端（203.0.113.5:37118）请求负载均衡器（1.1.1.1:80），四层负载均衡器根据调度算法挑选了某个后端服务器（10.0.0.2:8080）处理请求。此刻，四层负载均衡器处理请求和响应的逻辑如下：
- 当客户端请求到达负载均衡器时，负载均衡器执行 NAT 操作：
	- 首先是 DNAT（目标地址转换） 操作：将目标 IP 和端口（1.1.1.1:80）改为后端服务器的 IP 和端口（10.0.0.2:8080），这使得**请求能够被路由至指定的后端服务器处理**。
	- 为了保持通信的完整性，负载均衡器还会执行 SNAT（源地址转换）操作。也就是原始源 IP 和端口（203.0.113.5:37118）改为四层负载均衡器的 IP 和端口（1.1.1.1:某个随机端口）。**SNAT 操作确保后端服务器认为请求是来自负载均衡器**，而不是直接来自客户端。
- 当后端服务器返回响应时，负载均衡器执行相反的 NAT 操作:
	- 将源 IP 和端口改回 1.1.1.1:80
	- 将目标 IP 和端口改回客户端的 203.0.113.5:37118

最终，客户端请求/接收的都是负载均衡器的 IP 和端口，而不知道实际的后端服务器信息。

从上述可见，NAT 模式下，负载均衡器代表整个服务集群接收和响应请求，当流量压力较大时，系统的瓶颈就很容易体现在负载均衡器上。
