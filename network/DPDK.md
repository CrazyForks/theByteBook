# 3.4.1 数据平面开发套件 DPDK

2010 年，由 Intel 领导的 DPDK（Data Plane Development Kit，数据平面开发套件）实现了一套基于“内核旁路”思想的高性能网络应用开发解决方案，并逐渐成为了独树一帜的成熟技术体系。

不同于 Linux 系统以通用性设计为目的，DPDK 为 Intel 处理器架构提供了库函数和驱动的支持，专注于网络应用中数据包的高性能处理。**也就是说 DPDK 绕过了 Linux 内核协议栈对数据包的处理过程，在用户空间直接对数据包进行收发与处理**。


图 3-17 展示了 DPDK 与传统内核网络的对比。在 Linux 系统看来，处于用户空间内的 DPDK Lib 和 APP 就是一个普通用户空间中的进程，它的编译、连接和加载方式和普通程序没有什么两样。但两者网络数据包在 Linux 系统中的传输路径完全不同：

- 左边是原内核方式：网络数据包遵循从网络接口卡（NIC）到驱动程序，再到内核协议栈，然后通过 Socket 接口传递给业务逻辑的路径。
- 右边是 DPDK 方式：网络数据包则基于用户空间 I/O（UIO）技术，绕过了内核协议栈，直接从网卡传输到 DPDK 基础库，然后传递给业务逻辑的路径。

:::center
  ![](../assets/dpdk.png)<br/>
 图 3-17 DPDK 与传统内核网络对比
:::

目前，负载均衡、金融量化交易和分布式机器学习等网络密集型系统，已经广泛应用 DPDK 技术。DPDK 通过跨越内核、直接在用户态处理数据包，从而充分利用硬件资源、减少处理延迟，实现了单机处理千万级并发连接，并达到了微秒级别的业务延迟性能指标。

如图 3-18 所示，基于 DPDK 的高性能四层负载均衡器 DPVS 与 LVS（Linux Virtual Server）的四层负载均衡在基准测试中的表现对比。从 PPS（Packet Per Second，包/秒）转发指标来看，DPVS 的性能比 LVS 高出 300%。

:::center
  ![](../assets/dpvs-performance.png)<br/>
 图 3-18 DPVS 与 LVS 的 PPS 性能指标对比 [图片来源](https://github.com/iqiyi/dpvs)
:::

对于海量用户规模的互联网应用来说，动辄需要部署数千、甚至数万台服务器，**如果能将单机性能提升十倍甚至百倍，无论是从硬件投入还是运营成本上来看都能带来非常可观的成本削减**，这样的技术变革带来的潜在效益非常诱人。