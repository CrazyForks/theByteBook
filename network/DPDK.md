# 3.4.1 数据平面开发套件 DPDK

2010 年，由 Intel 领导的 DPDK实现了一套基于“内核旁路”思想的高性能网络应用解决方案，并逐渐成为了独树一帜的成熟技术体系。

起初，DPDK （Data Plane Development Kit，数据平面开发套件）实际上是 Intel 为了卖自家的硬件，针对 Intel 处理器和网卡开发的一款高性能的网络驱动组件。DPDK 开源之后，越来越多的厂商参与进来贡献代码，DPDK 开始支持更多的硬件。如处理器不仅支持 Intel，还支持 AMD、ARM 等厂商的处理器；网卡支持的范围也包括 Intel 网卡、Mellanox 网卡、ARM 集成网卡等。

图 3-6 展示了 DPDK（Fast Path）与传统内核网络（Slow Path）的区别。在 Linux 系统中，位于用户空间的 DPDK Lib 和 APP 的编译、连接和加载方式和普通程序没什么区别。但两者网络数据包在 Linux 系统中的传输路径完全不同：

- 左侧展示的是传统内核方式：网络数据包自网络接口卡（NIC）出发，经过驱动程序，内核协议栈，最后通过 Socket 接口传递至业务逻辑。
- 右侧则展示的是 DPDK 方式：在该方案中，网络数据包利用用户空间 I/O（UIO）技术，直接绕过内核协议栈，从网卡转移到 DPDK 基础库，然后传递至业务逻辑。也就是说 DPDK 绕过了 Linux 内核协议栈对数据包的处理过程，在用户空间直接对数据包进行收发与处理。

:::center
  ![](../assets/dpdk.png)<br/>
 图 3-6 DPDK 与传统内核网络对比
:::


图 3-7 展示了 DPVS（基于 DPDK 实现的四层负载均衡器）与 LVS（基于 Linux 内核网络实现的四层负载均衡器）的性能对比。根据每秒转发数据包数量（Packet Per Second，PPS）指标来看，DPVS 的表现比 LVS 高出 300%。

:::center
  ![](../assets/dpvs-performance.png)<br/>
 图 3-7 DPVS 与 LVS 的 PPS 性能指标对比 [图片来源](https://github.com/iqiyi/dpvs)
:::

对于海量用户规模的互联网应用来说，动辄需要部署数千、甚至数万台服务器，如果能将单机性能提升十倍甚至百倍，无论是从硬件投入还是运营成本上来看，都能带来非常可观的成本削减。这样的技术变革带来的潜在效益，无疑非常诱人。

DPDK 属于硬件厂商主导的“内核旁路”技术。接下来，笔者再介绍由社区开发者主导的另一类“内核旁路”技术。