# 3.4.1 数据平面开发套件 DPDK

2010 年，由 Intel 领导的 DPDK（Data Plane Development Kit，数据平面开发套件）实现了一套基于“内核旁路”思想的高性能网络应用开发解决方案，并逐渐成为了独树一帜的成熟技术体系。

起初，DPDK 实际上是 Intel 为了卖自家的硬件，针对 Intel 处理器和网卡开发的一款高性能的网络驱动组件。DPDK 开源之后，越来越多的厂商参与进来贡献代码，这使得 DPDK 支持更多的处理器和网卡。例如，处理器不仅支持 Intel，还支持 AMD、ARM 等厂商的处理器；网卡支持的范围也包括 Intel 网卡、Mellanox 网卡、ARM 集成网卡等。

图 3-6 展示了 DPDK 与传统内核网络的区别。在 Linux 系统中，位于用户空间的 DPDK Lib 和 APP 被视为一个普通的用户进程，其编译、连接和加载方式和普通程序没什么区别。但两者网络数据包在 Linux 系统中的传输路径完全不同：

- 左侧展示的是传统内核方式：网络数据包自网络接口卡（NIC）出发，经过驱动程序，内核协议栈，最后通过 Socket 接口传递至业务逻辑。
- 右侧则展示的是 DPDK 方式：在该方案中，网络数据包利用用户空间 I/O（UIO）技术，直接绕过内核协议栈，从网卡转移到 DPDK 基础库，然后传递至业务逻辑。也就是说 DPDK 绕过了 Linux 内核协议栈对数据包的处理过程，在用户空间直接对数据包进行收发与处理。

:::center
  ![](../assets/dpdk.png)<br/>
 图 3-6 DPDK 与传统内核网络对比
:::


图 3-7 展示了基于 DPDK 的高性能四层负载均衡 DPVS 与基于 Linux 内核网络的 LVS 四层负载均衡性能对比。从每秒转发数据包数量（PPS）这一指标来看，DPVS 的表现比 LVS 高出 300%。对于海量用户规模的互联网应用来说，动辄需要部署数千、甚至数万台服务器，如果能将单机性能提升十倍甚至百倍，无论是从硬件投入还是运营成本上来看都能带来非常可观的成本削减，这样的技术变革带来的潜在效益非常诱人。

:::center
  ![](../assets/dpvs-performance.png)<br/>
 图 3-7 DPVS 与 LVS 的 PPS 性能指标对比 [图片来源](https://github.com/iqiyi/dpvs)
:::


