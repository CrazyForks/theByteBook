# 3.4.3 内核旁路技术 DPDK

通过本章介绍的 Linux ingress 架构以及内核协议栈优化技术，相信读者已经认识到“**高并发环境下，网络协议栈的复杂处理，以及内核态与用户态的频繁切换，往往会成为主要的性能瓶颈**”，这一点在网络密集型系统中尤为明显，因为内核处理能力的局限性直接制约了整个系统的性能表现。

在人们想办法提升内核性能的同时，另外一批人抱着它不行就绕开它的思路，在 2010 年，由 Intel 领导的 DPDK 实现了一套基于内核旁路（Kernel bypass，跳过内核，完全在用户空间做全部的包处理）思想的高性能网络应用开发解决方案，并逐渐成为了独树一帜的成熟技术体系。


:::tip 什么是 DPDK

DPDK（Data Plane Development Kit，数据平面开发套件）为 Intel 处理器架构下用户空间高效的数据包处理提供了库函数和驱动的支持，它不同于 Linux 系统以通用性设计为目的，而是专注于网络应用中数据包的高性能处理。

也就是说 DPDK 绕过了 Linux 内核协议栈对数据包的处理过程，在用户空间直接对数据包进行收发与处理。

:::

图 3-17 展示了 DPDK 与传统内核网络的对比。在 Linux 系统看来，处于用户空间内的 DPDK Lib 和 APP 就是一个普通的用户态进程，它的编译、连接和加载方式和普通程序没有什么两样。

但两者的数据包在 Linux 系统中的传输路径却有本质的不同：

- 左边是原内核方式：数据流遵循从网络接口卡（NIC）到驱动程序，再到内核协议栈，然后通过 Socket 接口传递给业务逻辑的路径。
- 右边是 DPDK 方式：数据流则采用基于用户空间 I/O（UIO）技术的路径，绕过内核协议栈，直接从网卡传输到 DPDK 基础库，最终送达业务逻辑。

:::center
  ![](../assets/dpdk.png)<br/>
 图 3-17 DPDK 与传统内核网络对比
:::

当前，业内已经有很多负载均衡系统使用 DPDK 或 eBPF 技术进行跨内核，直接在用户态进行数据包的处理。例如 Facebook 的 Katran、美团的 MGW、爱奇艺的 DPVS 系统等等，正是基于此，这些系统能够充分利用硬件资源，减少处理延迟，实现单机处理千万级别并发连接的性能指标。

如图 3-18 所示的四层负载均衡基准测试，DPVS 与 LVS 在 PPS 转发上的指标对比，DPVS 性能提升约 300%。

:::center
  ![](../assets/dpvs-performance.png)<br/>
 图 3-18 DPVS 与 LVS 的 PPS 性能指标对比 [图片来源](https://github.com/iqiyi/dpvs)
:::

对于海量用户规模的互联网应用来说，动辄需要部署数千、甚至数万台服务器，**如果能将单机性能提升十倍甚至百倍，无论是从硬件投入还是运营成本上来看都能带来非常可观的成本削减**，这样的技术变革带来的潜在效益非常诱人。

