# 3.4.3 内核旁路技术 DPDK

本书 3.3.3 节介绍的 XDP 实际属于**半内核旁路技术**，这一节，我再介绍一种**完全的内核旁路技术**。

经前辈先驱们的研究，目前业内已经出现了很多优秀内核旁路思想的高性能网络数据处理框架，如 6WIND、Wind River、Netmap、DPDK 等。

其中，Intel 的 DPDK 在众多方案脱颖而出，一骑绝尘。

:::tip DPDK

DPDK（Data Plane Development Kit，数据平面开发套件）为 Intel 处理器架构下用户空间高效的数据包处理提供了库函数和驱动的支持，它不同于 Linux 系统以通用性设计为目的，而是专注于网络应用中数据包的高性能处理。

也就是 DPDK 绕过了 Linux 内核协议栈对数据包的处理过程，在用户空间实现了一套数据平面来进行数据包的收发与处理。

:::

图 3-17 展示了 DPDK 与传统内核网络的对比，在内核看来，DPDK 就是一个普通的用户态进程，它的编译、连接和加载方式和普通程序没有什么两样：

- 左边是原内核方式：数据从网卡 -> 驱动 -> 协议栈 -> Socket 接口 -> 业务。
- 右边是 DPDK 方式：基于 UIO（Userspace I/O）旁路数据。数据从网卡 -> DPDK 轮询模式-> DPDK 基础库 -> 业务。

:::center
  ![](../assets/dpdk.png)<br/>
 图 3-17 DPDK 与传统内核网络对比
:::

很多企业如 Facebook 的 Katran、美团的 MGW、爱奇艺的 DPVS 等使用 DPDK 或 eBPF 技术进行跨内核，直接全部在用户态进行数据包的处理，正是基于此，得以实现单机千万并发的性能指标。

如图 3-18 所示的 L4 负载均衡基准测试，DPVS 与 LVS 在 PPS 转发上的指标对比，dvps 性能提升约 300%。

:::center
  ![](../assets/dpvs-performance.png)<br/>
 图 3-18 dpvs 性能指标对比 [图片来源](https://github.com/iqiyi/dpvs)
:::

对于海量用户规模的互联网应用来说，动辄需要部署数千、甚至数万台服务器，**如果能将单机性能提升十倍甚至百倍，无论是从硬件投入还是运营成本上来看都能带来非常可观的成本削减**，这样的技术变革带来的潜在效益非常诱人。

