# 3.5.3 远程直接内存访问 RDMA

当计算从单机到分布式，从同构到异构转变时，不同计算节点之间、异构 xPU（如 CPU、GPU、DPU）之间，超高带宽、超低延迟、超可靠的互联技术就成为高性能计算的迫切需求。

面对庞大的数据交换，原本用于高性能计算领域的 RDMA（Remote Direct Memeory Access，远程直接内存访问）技术成为低延迟、高吞吐量数据传输首选解决方案方案。

RDMA 的工作原理如下图所示，RDMA 绕过了传统以太网复杂的 TCP/IP 协议栈，直接从网卡硬件上开始网络数据传递，而且中间不需要 CPU 干预。一次跨主机的网络请求，就像读取本地的内存一样。

:::center
  ![](../assets/RDMA.png)<br/>
  图  RDMA 
:::

RDMA 网络主要由三种协议实现：Infiniband、RoCE 和 iWARP（iWARP 由于应用并不广泛，不再展开讨论）。

2000年，由 IBTA 提出的 Infiniband（无限带宽）是当之无愧的技术核心，其规定了一整套完整的链路层到传输层规范（不兼容现在有以太网）。使用 Infiniband 构建的 RDMA 网卡能实现时延小于 3us，网络吞吐 400Gb/s + 的极致网络（以太网最高只有 100 Gb/s）。极致性能的优势让 Infiniband 成为超级计算机方案的首选，风靡全球的人工智能应用 ChatGPT 也是基于英伟达公司的 InfiniBand 技术构建的。

但极致性能的代价是构建 Infiniband 网络需要全套的网络设备支持，需要专用的网卡、专用的交换机、专用的线缆等，技术方案/设备被厂商锁定、价格昂贵以及非通用协议一系列的缺陷。

为了降低 RDMA 使用成本以及使 RDMA 技术走向通用数据中心领域，2010 年 IBTA 发布了 RoCE（RDMA over Converged Ethernet，基于融合以太网的远程直接内存访问）技术，将 Infiniband 的四层传输协议 RDMA“移植”到以太网。

RoCE 的发展其实涉及了 2 个版本：
- 早期的 RoCEv1 使用了 IB 规范，仅支持在二层的以太网实现 RDMA 传输，由于场景和网络规模受限，并没有得到广泛推广。
- 直到 RoCEv2 出现，传输层的 RDMA 被封装在 UDP 协议之内，解除了 RoCEv1 无法跨子网的限制后，结合本身固有的低成本以及兼容性优势，RoCE 开始被广泛应用于 AI 训练、分布式存储、并行计算等通用数据中心中的场景。

:::center
  ![](../assets/RoCE_Header_format.png)<br/>
  图 RoCE v1 只能在广播域内通信，RoCE v2 支持 L3 路由
:::

但注意的是，RoCE 的设计源于 InfiniBand 无损网络，构筑在 UDP 协议之上，因此缺乏完善的丢包保护机制，任意一个报文的丢失都会造成大量的重传，严重影响数据传输性能。这就要求在 L2 网络内数据包传输进行控制，通过实现无损的以太网传输来保证数据传输的可靠性。

随着 RDMA 走向通用数据中心，各种 RoCE 拥塞控制算法不断被提出，例如微软和 Mellanox 提出的 DCQCN、阿里提出 HPCC（High Precision Congestion Control）算法，这些新一代网络拥塞控制协议确保了高速网络的极致性能和超高稳定性，真正将 RDMA 技术推进至通用数据中心。
