# 3.2 Linux 系统收包流程

设计一个面向海量用户规模的系统时，我们要能够理解性能瓶颈，进行持续的性能优化。很多时候，如果对 Linux 系统一些关键设计（如收包流程）掌握不全面、理解不深的话，遇到性能瓶颈问题时，你就会觉得狗拿刺猬，无从下手。

本节，我们了解数据包进入网卡（eth0）后，Linux 系统中的各个组件如何相互协作的，深入理解 Linux 系统下网络包的接收全过程。Linux 系统收包流程如图 3-1 所示。
:::center
  ![](../assets/networking.svg)<br/>
图 3-1 Linux 系统收包流程
:::

1. 当外部网络发送数据包到服务器时，首先由网卡 eth0 接收该数据包。
2. 网卡通过 DMA（Direct Memory Access，直接内存访问） 技术，不经过 CPU 的干预，将数据包直接拷贝到内核中的 RingBuffer（环形缓冲区）等待 CPU 处理。RingBuffer 是一种首尾相接的环形数据结构，它的主要作用是作为缓冲区，缓解网卡接收数据的速度快于 CPU 处理数据的速度问题。
3. 数据包成功写入 RingBuffer 后，网卡会产生 IRQ（Interrupt Request，硬件中断），通知内核有新的数据包到达。
4. 内核收到硬件中断后，立即调用对应的中断处理函数。通常情况下，中断处理函数会简单地标记有新数据到达，并唤醒 ksoftirqd 内核线程来处理软中断（SoftIRQ）。
5. 在 ksoftirqd 线程被唤醒后，内核会进行软中断处理。这时，内核会调用网卡驱动在内核中注册的 NAPI（New API）poll 接口，从 RingBuffer 中提取数据包，并生成 skb（Socket Buffer）数据。skb 是 Linux 内核中用于管理网络数据包的主要结构。它包含了网络包的所有信息，包括头部、数据负载等，并在内核的各个网络协议层之间传递。
6. skb 被传递到内核协议栈中进行处理。这里涉及多个网络层次的处理操作：
	- 网络层（L3 Network layer）：例如 IP 层会根据主机中的路由表，判断数据包路由到哪一个网络接口（Network Interface）。这里的网络接口可能是稍后介绍的虚拟设备，或者是物理网卡 eth0 接口。
	- 传输层（L4 Transport layer）：例如 TCP 层会解封装数据包，处理网络地址转换（NAT）、连接跟踪（conntrack）等任务。
7. 在内核协议栈处理完成后，将数据包传递到 socket 接收缓冲区。应用程序随后利用系统调用（如 Socket API）从这个缓冲区中读取数据，完成数据的接收过程。


分析 Linux 系统处理网络数据包的过程，我们可以注意到潜在的问题：数据包的处理流程过于冗长。整个处理流程涉及到多个层级的协议栈，如数据链路层、网络层、传输层和应用层。这些层级之间需要进行多次上下文切换和拷贝操作，从而导致延迟增加。

对于多数常规应用而言，Linux 内核的局限性并不需要关注。然而，需要处理大规模并发连接的密集网络系统时，Linux 内核造成的瓶颈就变得不可忽视。除了想办法优化内核网络协议栈，业界也出现了“绕过内核”这一思想的技术。例如，图 3-1 中的 XDP 和 DPDK 技术，笔者将在 3.4 节详细介绍它们的区别以及原理。

接下来，我们将继续深入 Linux 内核网络框架部分，研究数据包在内核协议栈中是如何被过滤、修改和转发的。