# 3.1 Linux 系统收包流程

如果设计一个 C10K（单机 1 万并发）指标的系统，无需在乎内核的细节，但如果要设计一个 C10M（单机 1000 万并发）的系统，那就不能忽略 Linux 内核带来的各种影响，这些影响包括用户进程调用系统进入内核态的开销，响应数据包时产生的硬中断开销，以及 ksoftirqd 处理软中断而产生的上下文开销等等。

你是否还记得笔者在第二章序言提到的面试题：“浏览器打开 url 到页面展现，中间发生了什么？”。这道题还没有结束，这一节我们了解 Linux 系统收包的流程。

<div  align="center">
	<img src="../assets/networking.svg" width="650"  align=center />
	<p>图3-2 Linux ingress 架构概览 </p>
</div>

如图 3-2 所示，Linux 系统收包流程如下：

1. 网卡 eth0 收到数据包。
2. 网卡通过 DMA（Direct Memory Access，直接内存访问）将数据包拷贝到内核 Ring Buffer（环形缓冲区），如果 Ring Buffer 满了则产生丢包 。
3. 网卡产生 IRQ（Interrupt ReQuest，硬件中断）告知内核有新的数据包达到。
4. 内核收到中断后, 调用相应中断处理函数，开始唤醒 ksoftirqd 内核线程处理软中断。
5. 内核进行软中断处理，调用驱动注册在内核中的 NAPI poll 接口从 Ring Buffer 中获取数据，并生成 skb（Socket Buffer），送至内核协议栈处理。
6. 内核中网络协议栈：L3 处理。
7. 内核中网络协议栈：L4 处理。
8. 网络协议栈处理数据后，并将其发送到对应应用的 socket 接收缓冲区。


大规模高并发的核心挑战**是用户态内核态的频繁转换、网络协议栈的冗长流程，也就是说内核才是高并发下导致瓶颈的原因所在**。


Linux ingress 架构介绍已经结束，Netfilter 框架以及内核旁路技术还在继续等待我们探索，下一节，我们进入内核网络细节。