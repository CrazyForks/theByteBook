# 3.2 Linux 系统收包流程


这一节，我们通过图 3-1 所示的 Linux ingress（Linux 接收数据包）架构，系统地了解网络数据包进入网卡（eth0）后，Linux 内核是如何处理的。


:::center
  ![](../assets/networking.svg)<br/>
图 3-1 Linux 系统收包过程
:::

根据 Linux ingress 架构，总结 Linux 系统收包过程如下：

1. 网卡 eth0 收到数据包。
2. 网卡通过 DMA（Direct Memory Access，直接内存访问）将数据包拷贝到内核 RingBuffer（环形缓冲区），如果 RingBuffer 满了则产生丢包 。
3. 网卡产生 IRQ（Interrupt ReQuest，硬件中断）告知内核有新的数据包达到。
4. 内核收到中断后, 调用相应中断处理函数，开始唤醒 ksoftirqd 内核线程处理软中断。
5. 内核进行软中断处理，调用驱动注册在内核中的 NAPI poll 接口从 RingBuffer 中获取数据，并生成 skb（Socket Buffer），送至内核协议栈处理。
6. 内核中网络协议栈处理：这里对数据包进行各种逻辑处理。例如：在网络层判断路由，在传输层进行数据包的解封/封装、网络地址转换（NAT）、连接跟踪（conntrack）等等。
7. 数据交付：网络协议栈处理数据后，并将其发送到对应应用的 socket 接收缓冲区。


通过对 Linux 系统处理网络数据包的过程进行分析，你是否注意到一些潜在的问题：数据包的处理流程过于冗长。对于多数常规应用而言，无需关注。然而，面对需要处理大规模并发连接的密集网络系统时，内核造成的瓶颈就变得不可忽视。除了想办法优化内核网络协议栈，业界也出现了“绕过内核”这一思想的技术，例如本书稍后介绍的 XDP、DPDK、RDMA 等技术。

接下来，我们继续深入 Linux 内核网络框架部分，研究数据包在内核协议栈中是如何被过滤、修改和转发的。理解 Linux 系统在处理大规模并发连接时面临的挑战，以及讨论如何应对这种挑战。