# 7.5 存储系统设计的演进

在计算机体系中，与 CPU 和 Mem 这些资源相比，硬盘被定义为外设，如果要使用，需要进行挂载。

```shell
$ mount /dev/hda1 /mnt
```

容器源于对操作系统的虚拟化，为满足容器对存储的需求，Volume 和 Mount 的设计自然也延续到容器系统中。 


Kubernetes抽象出了 Volume 来解决存储的问题，集群中的每一个 Volume 在被 Pod 使用时都会经历四个操作，也就是附着（Attach）、挂载（Mount）、卸载（Unmount）和分离（Detach）。



观察 Kubernetes 存储系统设计的演进可以略见一斑，为什么有如此之多的概念。

- 从 Pod 中独立出来，具有单独的声明周期；
- 从临时存储到持久化存储
- PV 的创建从静态到动态
- 扩展的方式从 in-tree 到 out-tree 的转变。


## 从临时卷到持久化存储

临时的卷没有办法解决数据持久存储的问题，想要让数据能够持久化，首先就需要将 Pod 和卷的声明周期分离，这也就是引入持久卷 PersistentVolume(PV) 的原因。

PersistentVolume 是集群中的一种被管理员分配的存储资源，而 PersistentVolumeClaim 表示用户对存储资源的申请，它与 Pod 非常相似，PVC 消耗了持久卷资源，而 Pod 消耗了节点上的 CPU 和内存等物理资源。


PV 作为存储资源，主要包括存储能力、访问模式、存储类型、回收策略、后端存储类型等关键信息的设置。

```
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv1
spec:
  capacity:  #容量
    storage: 5Gi
  accessModes:  #访问模式
  - ReadWriteOnce
  persistentVolumeReclaimPolicy: Recycle  #回收策略
  storageClassName: slow  
  nfs:
    path: /
    server: 172.17.0.2
```

而对于开发人员讲，只想知道我有多大的空间、I/O 是否满足要求，并不关心存储底层的配置。为了解决这个问题，引入了 PVC（Persistent Volume Claim）。

```
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: task-pv-claim
spec:
  storageClassName: manual
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 3Gi
```


PVC 和 PV 的设计，其实跟“面向对象”的思想完全一致：
- PVC 可以理解为持久化存储的“接口”，它提供了对某种持久化存储的描述，声明需要的存储类型、大小、访问模式等需求；
- 而这个持久化存储的实现部分则由 PV 负责完成。

这样做的好处是，作为应用开发者，我们只需要跟 PVC 这个“接口”打交道，而不必关心底层存储实现是 NFS 还是 Ceph。


## 从静态到动态

PV 资源有两种供应的方式，一种是静态的，另一种是动态的：
- 静态存储供应要求集群的管理员预先创建一定数量的 PV，然后使用者通过 PVC 的方式对 PV 资源的使用进行声明和申请；
- 但是当系统管理员创建的 PV 对象不能满足使用者的需求时，就会进入动态存储供应的逻辑，供应的方式是基于集群中的 StorageClass 对象。



## 从 in-tree 到 out-tree 的转变

从 1.9 开始又引入了Container Storage Interface（CSI）容器存储接口

:::center
  ![](../assets/CSI.png)<br/>

  CNCF 下的 Kubernetes 存储生态
:::

上述众多的存储系统，提供的服务都均可划分为文件存储、块存储和对象存储三种类型，**三者的划分的依据可以根据数据的“用户”不同来解释：块存储的用户是可以读写块设备的软件系统，例如传统的文件系统、数据库；文件存储的用户是自然人；对象存储的用户则是其它计算机软件**。


### 块存储

传统的文件系统，是直接访问存储数据的硬件介质的。介质不关心也无法去关心这些数据的组织方式以及结构，因此用的是最简单粗暴的组织方式：所有数据按照固定的大小分块，每一块赋予一个用于寻址的编号。所以，硬盘往往又叫块设备（Block Device）。

在 Linux 的 IO 软件栈中，要直接使用块存储的话就要基于 LBA 编程，使用存储相匹配的协议（SCSI、SATA、SAS、FCP、FCoE、iSCSI..），因此接口较为简单朴素，再加上块存储本身处于整个存储软件栈的底层，这导致**块存储使用起来并不十分友好，但具有超低的时延和超高的吞吐**。

### 文件存储

文件存储的用户是自然人，这个最容易理解。

专门组织块结构来构成文件的块的表（比如FAT），在表中再加入其他控制信息，就能很方便地扩展出更多的高级功能，比如除了文件占用的块地址信息外，在表中再加上文件的逻辑位置就形成了目录，加上文件的访问标志就形成了权限，我们还可以再加上文件的名称、创建时间、所有者、修改者等一系列的元数据信息。

文件存储的访问不像块存储那样有五花八门的协议，其 POSIX 接口（Portable Operating System Interface，POSIX）已经成为事实标准，诸如 Open、Write、Read 等许多操作数据的接口都能在文件系统中被找到。人们把定义文件分配表应该如何实现、储存哪些信息、提供什么功能的标准称为文件系统（File System），很常用的文件系统如 FAT32、NTFS、exFAT、ext2/3/4、XFS、BTRFS 等等。


### 对象存储

对象存储其实介于块存储和文件存储之间。文件存储的树状结构以及路径访问方式虽然方便人类理解、记忆和访问，但计算机需要把路径进行分解，然后逐级向下查找，最后才能查找到需要的文件，对于应用程序来说既没必要，也很浪费性能。


而块存储是排它的，服务器上的某个逻辑块被一台客户端挂载后，其它客户端就无法访问上面的数据了。而且挂载了块存储的客户端上的一个程序要访问里面的数据，不算类似数据库直接访问裸设备这种方式外，通常也需要对其进行分区、安装文件系统后才能使用

是否可以用不排它但又类似块设备访问的方式呢？但对块设备的访问方式虽然比文件存储快，其实也很麻烦——一个文件往往是由多个块组成，并且很可能是不连续的。

为了解决这中麻烦，使用一个统一的底层存储系统，管理这些文件和底层介质的组织结构，然后给每个文件一个唯一的标识，其它系统需要访问某个文件，直接提供文件的标识就可以了。存储系统可以用更高效的数据组织方式来管理这些标识以及其对应的存储介质上的块。

当然，对于不同的软件系统来说，一次访问需要获取的不一定是单个我们传统意义上的文件，根据不同的需要可能只是一个/组值，某个文件的一部分，也可能是多个文件的组合，甚至是某个块设备，统称为对象。这就是对象存储。


至于如何选择
公有云的：AWS S3，腾讯云的 COS，阿里云的 OSS 等，HDFS、FastDFS、swift 等属于对象存储。

