# 使用 RDMA 构建高速网络

无论是利用 overlay 实现的隧道网络，还是 underlay 方式的主机网络直通，仅仅解决了容器网络“通与不通”的问题。

大规模 AI 集群中，百亿、千亿级别参数量的大模型得使用分布式训练，节点任务之间需要交换海量的参数梯度等信息，若使用“通与不通”的入门级手段，对 AI 训练这种百亿级参数传递而言实在太慢了。

为了加速任务间数据交换，原本用于高性能计算领域的 RDMA（Remote Direct Memeory Access，远程直接内存访问）技术成为 AI 应用性能优化的首选方案。

RDMA 的工作原理如下图所示，RDMA 绕过了传统以太网复杂的 TCP/IP 协议栈，直接从网卡硬件上开始网络数据传递，而且中间不需要 CPU 干预。一次跨主机的网络请求，就像读取本地的内存一样。

:::center
  ![](../assets/RDMA.png)<br/>
  图  RDMA 
:::

RDMA 网络主要由三种协议实现：Infiniband、RoCE 和 iWARP（iWARP 由于应用并不广泛，不再展开讨论）。

2000年，由 IBTA 提出的 Infiniband（无限带宽）是当之无愧的技术核心，其规定了一整套完整的链路层到传输层规范（不兼容现在有以太网）。使用 Infiniband 构建的 RDMA 网卡能实现时延小于 3us，网络吞吐 400Gb/s + 的极致网络（以太网最高只有 100 Gb/s）。极致性能的优势让 Infiniband 成为超级计算机方案的首选（值得一提的是 chatGPT 背后的网络是英伟达公司基于 InfiniBand 构建）

但代价是构建 Infiniband 网络需要全套的网络设备支持（专用的网卡、专用的交换机、专用的线缆），同时也存在被厂商锁定、价格昂贵以及非通用协议一系列的缺陷。

为了降低 RDMA 使用成本以及使 RDMA 技术走向通用数据中心领域，2010 年 4月 IBTA 发布了 RoCE（RDMA over Converged Ethernet，基于融合以太网的远程直接内存访问）技术，将 Infiniband 的四层传输协议 RDMA“移植”到以太网。

RoCE 的发展其实涉及了 2 个版本：
- 早期的 RoCEv1 使用了 IB 规范，仅支持在二层的以太网实现 RDMA 传输，由于场景和网络规模受限，并没有得到广泛推广。
- 直到 RoCEv2 出现，传输层的 RDMA 被封装在 UDP 协议之内，解除了 RoCEv1 无法跨子网的限制后，结合本身固有的低成本以及兼容性优势，RoCE 开始被广泛应用于 AI 训练、分布式存储、并行计算等通用数据中心中的场景。

:::center
  ![](../assets/RoCE_Header_format.png)<br/>
  图 RoCE v1 只能在广播域内通信，RoCE v2 支持 L3 路由
:::

但注意，RoCE 的设计源于无损的 InfiniBand 网络，并构筑于 UDP/IP 之上，因此缺乏完善的丢包保护机制，任意一个报文的丢失都会造成大量的重传，严重影响数据传输性能。

一种解决方案是，在 L2 对网络中的流传输进行控制，通过实现无损的以太网传输来保证数据传输的可靠性。另一种解决方案是增加 RoCE 协议的可靠性，如像 TCP 那样增加握手，以牺牲性能为代价提供可靠性。

业内目前大都采用第一种方案，即在不丢包的前提下，尽可能提高通信性能，也就是拥塞控制。随着 RDMA 走向通用数据中心，各种 RoCE 拥塞控制算法也在不断被提出，譬如微软和 Mellanox 提出的 DCQCN、阿里提出 HPCC（High Precision Congestion Control）。这些新一代高速云网络拥塞控制协议实现了高速网络的极致性能和超高稳定性，真正将 RDMA 技术推进至通用数据中心。

现在，假设你已经安装好 RoCE 网卡、驱动程序以及配置好无损网络。接下来的工作是将 RDMA 高速网络接入到容器内，这里面的课题包括 RoCE 网卡虚拟化、Pod 内多网卡。
