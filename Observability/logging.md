# 9.3.2 日志的存储与分析

处理日志本来是件稀松平常的事情，但在工程师们普遍爱写日志的影响下，日志处理质变为典型的大数据场景之一：高吞吐写入（GB/s）、低成本海量存储（PB 级别）、实时文本检索（1s 内）。

本节，我们从索引原理和成本分析的角度介绍 Elastic（全文索引）、Loki（仅索引元数据）和 ClickHouse（列式数据库）三类技术如何解决日志的存储/分析难题。

## 1. 传统解决方案 ELK

讨论实现一套完整的日志系统，工程师们或多或少都应该听说过这几个名词：ELK、ELKB 或者 Elastic Stack[^1]，这些其实说得都是同一套实现日志处理方案的开源组件。

图 9-10 展示了一套处理日志的 Elastic（为统一明确，本文统一称 Elastic）技术方案。这套方案中，Beats 部署到日志所在地收集原始数据，然后使用 MQ 做缓冲换取更好的吞吐，接着发给 logstash 做数据清洗，最后落地到 Elasticsearch 集群并进行索引，使用时通过 Kibana 来检索和分析，如果有必要挂上 Nginx 做各类访问控制。

:::center
  ![](../assets/ELK.png)<br/>
  图 9-10 整合了消息队列和 Nginx 的 Elastic 日志系统
:::

Elastic 套件中最核心组件是 Elasticsearch，这是一个提供一种“准实时”搜索服务的分布式搜索分析引擎。准实时的意思是，生产环境中可以做到数据上报 10 秒后可搜，不惜成本的情况下，万亿级规模下日志查询秒级响应。

:::tip 额外知识
说起 Elasticsearch 不得不提及背后的 Lucene。Lucene 的作者就是大名鼎鼎的 Doug Cutting，如果你不知道他是谁是？那你一定听过他儿子玩具的名字 —— Hadoop。

Lucene 是一个全文检索引擎，离直接使用还有部分集成工作，之后陆续有了 Solr、Nutch 等项目帮助发展，但依然不温不火。直到 2012 年，Elasticsearch 诞生后，通过优秀的 Restful API、分布式部署等扩展才把 Lucene 的使用推向新的高度。

:::

Elasticsearch 在**日志场景中的优势在于全文检索能力，快速从海量的数据中检索出关键词匹配的日志**，其底层核心是 Lucene 中的反向索引（Inverted index）技术。

索引有两种形式，分为正向索引和反向索引：

- 正向索引（Forward Index）
- 反向索引（inverted index）常被翻译为倒排索引，但倒排极容易误解为倒序，不如翻译**反向索引**。反向索引是信息检索领域常用的索引技术，原理将文本分割成一个个词，通过构建“<词->文档编号>”这样的索引，从而快速查找一个词在哪些文档出现。

以英文为例，下面是要被索引的文本：

- T~0~ = "it is what it is"
- T~1~ = "what is it"
- T~2~ = "it is a banana"

我们就能得到下面的反向文件索引：

```
"a":      {2}
"banana": {2}
"is":     {0, 1, 2}
"it":     {0, 1, 2}
"what":   {0, 1}
```

进行检索时，条件“what”, “is” 和 “it” 将对应这个集合：$\{0, 1\}\cap\{0,1,2\}\cap\{0,1,2\} = \{0,1\}$。

因此，Elasticsearch 搜索时不需要逐个扫描所有文档，而是能通过关键词快速定位包含这些词的文档。然而，面对海量数据集或者非常高的查询压力时，仅使用反向索引还不够。我们还需要将数据拆分成分区（partitions），也称为分片（sharding）。

分区的定义为，每一条数据（或者每条记录，每行，每个文档）只属特定的分区。实际上，每个分区都可以视为一个完整的数据库，在 Elasticsearch 中，分片是独立的 Lucene 实例。当文档写入时，Elasticsearch 会通过哈希函数（通常是基于文档 ID）计算出文档应该存储在哪个分区中，将文档有序分配到不同分区。

采用分区的目的是提高可扩展性，不同的分区可以放在不同的节点中。这样，查询负载也也随之分布到不同的机器上，并行地在多个分片上执行，然后将结果聚合后再返回给客户端，这极大地提高了查询吞吐量。

Elasticsearch 极致查询性能的背后，也付出了数据写入吞出率低和存储空间占用高的代价：

- 因为文档写入时需要进行分词、词典排序、构建排序表等 CPU/内存密集操作，这导致写入吞出率大幅下降；
- Elasticsearch 会存储原始数据和反向索引，为了加速分析可能还需要存储一份列式数据。
- 默认情况下，Elasticsearch 的每个分区都有一个冗余的副本。当分区的数据丢失时，冗余的副本就会顶上去成为新的分区。这样，避免了单点故障，从而实现容灾。

Elasticsearch 冗余数据机制在观测场景下导致极高的存储成本。如果你的目的只是把日志收集起来，作用是近期范围内查询和一些简单的条件匹配（如匹配 host、service 等），那不妨看看下面介绍的 Loki。

## 2. 日志处理新贵 Loki 

Loki 是 Grafana Labs 公司推出的类似于 Prometheus 的日志系统，官方的项目介绍是“like Prometheus，but for logs”。

Loki 明显优势是非常经济，它不再根据日志原始内容建立大量的全文索引，而是借鉴了 Prometheus 核心的思想 **使用标签对日志进行特征标记，只索引与日志相关的元数据标签，日志内容不做任何索引，并压缩存储于对象存储中**。相较于 Elastic 全文索引系统，Loki 的最终存储成本可降低数十倍甚至更低。

Loki 另外一个特点是对以 Kubernetes 为基座的系统十分友好。如图 9-11 所示，日志收集组件 Promtail 以 DaemonSet 方式运行在每个节点中，负责收集日志并将其发送给 Loki。

因为日志数据使用和 Prometheus 一样的标签来作为索引，通过这些标签，既可以查询日志的内容，也可以查询到监控的内容，还能对接到 alertmanager。

:::center
  ![](../assets/loki-arc.png)<br/>
  图 9-11 Loki 与 Kubernetes 密切集成
:::

作为 Grafana Labs 的自家产品，Loki 与 Grafana 密切集成，利用 Loki 的查询语法 LogQL 使用标签及运算符进行过滤，能展示出任何你想要的图表。

:::center
  ![](../assets/loki-dashboard.jpeg)<br/>
  图 9-12 在 Grafana 中通过 LogQL 查询展示不同的图表
:::

最后，Loki 和 Elastic 都是优秀的日志解决方案，如何选择取决于具体场景：
- Loki 相对轻量，具有较高的可扩展性和简化的存储架构，若是数据的处理不那么复杂，且有时序属性，如应用程序日志和基础设施指标，并且以 Kubernetes 为底座的 系统时，选择 Loki 更合适。
- Elastic 相对重量，需要复杂的存储架构和较高的硬件要求，部署和管理也比较复杂，适合更大的数据集和更复杂的数据处理需求。

## 3. 凶猛彪悍的 ClickHouse

最近几年， ClickHouse 是一个用于 OLAP（On-Line Analytical Processing，在线分析处理）领域的列式列式数据库管理系统

ClickHouse 的关键特点有列式存储、向量化查询执行、高效压缩、高性能、实时数据处理、水平扩展、复杂查询（支持 SQL 语法）。

这些特点使 ClickHouse 大规模数据分析、实时流式数据查询以及业务数据分析的理想选择。

一个流行的观点认为：“提升查询速度的最简单有效方法是减少数据扫描范围和数据传输量”。


针对分析类的查询，通常只需要读取表的一小部分列。


传统的行式数据库系统中，如 MySQL、Postgres，数据按如下顺序存储。

<center >表 9-13 行式数据库存储结构</center>

|Row | ProductId |sales  |Title| GoodEvent |CreateTime|
|:--|:--|:--|:--|:--|:--|
| #0 | 89354350662 |120 |Investor Relations|  1 |2016-05-18 05:19:20|
| #1 |  90329509958 | 10|  Contact us |  1 | 2016-05-18 08:10:20| 
| #2 |  89953706054 | 78 | Mission|  1 | 2016-05-18 07:38:00| | 
| #N |  ...|  ...|  ...|  ... | ...


行式数据库一张表中的一行内的所有数据在物理介质内是彼此相邻存储的。如果要执行下面的 SQL：

```
SELECT sum(sales) FROM 表 WHERE  ProductId=90329509958
```

首先需要将所有行从磁盘加载到内存中，然后进行扫描和过滤（检查是否符合 where 条件），过滤出目标行之后，再判断是否有聚合函数，如有则执行相应的计算和排序，最终输出结果。整个流程可能需要非常长的时间。


在列式数据库系统中，数据按如下的顺序存储：

|Row:| #0 | #1 | #2 | #N|
|:--|:--|:--|:--|:--|
|ProductId:| 89354350662 |90329509958 |89953706054 |...|
|JavaEnable: |1 |0| 1 |...|
|Title: | Investor  Relations | Contact us | Mission |...|
|GoodEvent: | 1| 1| 1| ...|
|CreateTime: | 2016-05-18 05:19:20 |2016-05-18 08:10:20 |2016-05-18 07:38:00 |...|

可以看到，列式存储不是讲一行中的所有值存储在一起，而是将每列中的所有值存储在一起

在列式数据库中你可以只读取你需要的数据。例如，如果只需要读取100列中的5列，这将帮助你最少减少20倍的I/O消耗。

此外，列式存储和数据压缩通常是伴生的，数据压缩的本质是通过一定的步长对数据进行匹配扫描，发现重复部分后进行编码转换。因此，数据中重复项越多，压缩率越高。面向列式存储，同一列字段的数据具有相同的数据类型和语义，重复项的可能性自然更高。按列配置压缩编解码器。我们决定为所有列保留默认的 LZ4 压缩。我们对 DateTime 列使用了 Double-Delta，对 Float 列使用了 Gorilla，对固定大小的 String 列使用了 LowCardinality。

近几年来，经常能在国内各个技术公众号看到关于列式数据库的实践分享，图 9-15 中的数据来源技术文章《B站基于Clickhouse的下一代日志体系建设实践》，文章中的数据表明 B 站使用 ClickHouse 降低了 60%+ 的存储成本[^2]

:::center
  ![](../assets/es-vs-clickhouse.png)<br/>
  图 9-15 同一份日志在 Elasticsearch、ClickHouse 和 ClickHouse(zstd) 中的容量对比
:::

根据 Yandex 的内部测试结果，ClickHouse 表现出了惊人的查询性能，从它的跑分结果来看（图 9-16），ClickHouse 比 Vertia（一款商业的 MPP 分析软件）快约 5 倍、比 Hive 快 279 倍、比 MySQL 快 801 倍。当之无愧阐述 ClickHouse 介绍中“实时”（real-time）二字含义，正如 ClickHouse 的宣传所言，其他的开源系统太慢、商用太贵，只有 ClickHouse 在成本与性能之间做到了良好平衡，又快还开源。


:::center
  ![](../assets/ClickHouse-benchmark.jpeg)<br/>
  图 9-16 ClickHouse 性能测试 [图片来源](http://clickhouse.yandex/benchmark.html)
:::

[^1]: Elastic 的发展始于 Shay Banon 的个人兴趣，从开源、聚人、成立公司，到走向纽交所，再到股价一路狂飙（最新市值 $107 亿），几乎是最理想的工程师创业故事。
[^2]: 参见 https://mp.weixin.qq.com/s/dUs7WUKUDOf9lLG6tzdk0g