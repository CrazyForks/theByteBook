# 8.5 ServiceMesh 的未来

基于 Istio 的 ServiceMesh 架构在互联网公司进行大规模线上部署的时候逐渐遇到以下问题：

- Envoy 上手难度大、维护成本高。
- Sidecar 带来的额外资源开销以及 Proxy 模式带来的延迟增加。
- xDs 全量下发带来的内存消耗过大以及配置文件过多，出现问题难以排查。


Envoy 作为 Sidecar 其提供的核心功能可以总结为以下三点：

- 对业务透明的请求拦截。
- 对拦截请求基于一定的规则进行校验、认证、统计、流量调度以及路由等。
- 将请求转发出去。

ServiceMesh 中所有的流量出入都要经过 Sidecar。Sidecar 的加入增加了业务请求的链路长度，那必然会带来性能损耗。但细观察以上三点，其中第1、3点都是与业务无关的，属于通用型功能，而第2点的性能是与业务复杂度强相关的，比如请求校验规则的多与少、遥测数据的采样率等等。

因为最有可能提升 Sidecar 性能的点就是高效处理对请求的拦截以降低 Sidcar 之间通信的延迟。

## 跨内核处理

针对请求的拦截，目前常规的做法是使用 iptbales，在部署 Sidecar 时配置好 iptbales 的拦截规则，当请求来临后 iptables 会从规则表中从上之下按顺序一条条进行匹配。如果遇到匹配的规则，那就执行本规则并根据本规则的动作（accept、reject、log 等），决定下一步的执行情况。

了解 iptbales 的基本流程之后，不难发现其中性能的瓶颈：

- iptables 的规则匹配是线性的，匹配的时间复杂度是 O(N)，当规则较多时候，性能会严重下滑。
- 每个请求的处理都要经过内核态到用户态再到内核态的过程。当网络中有大量数据包到达内核时，会产生频繁的硬件中断、上下文切换以及数据在内核态和用户态来回拷贝的性能损耗。

针对以上的两点性能瓶颈，Linux 社区以及 Envoy 社区通过以下方式进行优化：

Linux 社区发布了 bpfilter，这是一个使用 Linux BPF 提供的高性能网络过滤内核模块，计划用来替换 netfilter 作为 iptables 的内核底层实现，实现 Linux 网络包过滤框架从 netfilter 向 BPF 过渡的演进。

其次，Envoy 社区也在推动官方重构其架构，实现用户自定义 network socket 实现。最终目的为了添加对 VPP（Vector Packt Processing）扩展支持。

无论是使用 VPP 或者 Cilium 都可以实现数据包在纯用户态或者内核态的处理，避免来回的拷贝、上下文切换，且可以绕过 Linux 协议栈，提高报文转发效率。进而达到提升请求拦截效率的目的。

## QUIC


## 高效通信

## xDS 全量下发的问题

当前 Istio 下发 xDS 使用的是全量的下发策略，也就是网络里面所有的 Sidecar 内存里都会有整个网格内所有的服务发现数据。这种方式在规模化的场景中，往往会出现性能瓶颈，主要体现在 xDS 的全量下发和频繁低效变更。

设计 xDS 按需下发的方案，即下发的 xDS 数据一定是 Sidecar 所需要的，避免非必要的冗余数据和无效变更。提升服务网格的整体性能，满足规模化落地场景的需要。

这里的按需下发主要是通过微服务调用的依赖关系，通过手动或者程序自动收集的方式来收集。

